{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "demanding-issue",
   "metadata": {},
   "source": [
    "# Method 1 (PLUMED masterclass 21-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "through-twelve",
   "metadata": {},
   "source": [
    "## 1. Setting things up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viral-particle",
   "metadata": {},
   "source": [
    "Here we first clear the outputs in case that this notebook needs to be rerun. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "median-elevation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COLVAR\n",
      "HILLS_LAMBDA\n",
      "Method_1.ipynb\n",
      "histograms\n",
      "plumed.dat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm: histograms/*: No such file or directory\n",
      "rm: fes*: No such file or directory\n",
      "rm: bck*: No such file or directory\n",
      "rm: weights.dat: No such file or directory\n",
      "rm: *yaml: No such file or directory\n",
      "rm: plumed_reweight.dat: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "%%bash  \n",
    "rm -r histograms/* COLVAR* HILLS* fes* bck* weights.dat *yaml plumed_reweight.dat|| true  # to ignore error if the files do not exit\n",
    "cp ../input_files/* . || true  # to ignore error\n",
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satellite-gabriel",
   "metadata": {},
   "source": [
    "Below we import the required packages and set up the settings for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "detailed-agenda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specified-gravity",
   "metadata": {},
   "source": [
    "## 2. Reweight the data and generate histograms for block averaging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cosmetic-broadcasting",
   "metadata": {},
   "source": [
    "As a referene, the following is the content of the PLUMED input file used to run the alchemical metadynamics simulaiton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "clean-preview",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda: EXTRACV NAME=lambda\n",
      "  \n",
      "METAD ...\n",
      "ARG=lambda\n",
      "SIGMA=0.01     # small SIGMA ensures that the Gaussian approaximate a delta function\n",
      "HEIGHT=1.2388545199729883   # 0.5 kT\n",
      "PACE=10        # should be nstexpanded\n",
      "GRID_MIN=0     # index of alchemical states starts from 0\n",
      "GRID_MAX=5     # we have 6 states in total\n",
      "GRID_BIN=5     # 5 bins between 6 states\n",
      "TEMP=298       # same as ref_t\n",
      "BIASFACTOR=50   \n",
      "LABEL=metad    \n",
      "FILE=HILLS_LAMBDA\n",
      "... METAD\n",
      "\n",
      "PRINT STRIDE=10 ARG=lambda,metad.bias FILE=COLVAR\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "cat plumed.dat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aerial-retirement",
   "metadata": {},
   "source": [
    "Now we prepare the PLUMED input file for reweighting and generating histograms. As shown below, the block size was set by the `CLEAR` keyword in the `HISTOGRAM` action and the `STRIDE` keyword in the `DUMPGRID` action. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acting-immune",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "\n",
    "cat > \"plumed_reweight.dat\" << EOF\n",
    "lambda: READ FILE=COLVAR VALUES=lambda IGNORE_TIME IGNORE_FORCES\n",
    "  \n",
    "METAD ...\n",
    "\n",
    "ARG=lambda\n",
    "SIGMA=0.01\n",
    "HEIGHT=0     # kJ/mol\n",
    "PACE=50000000        # should be nstexpanded\n",
    "GRID_MIN=0\n",
    "GRID_MAX=5\n",
    "GRID_BIN=5\n",
    "TEMP=298\n",
    "BIASFACTOR=50\n",
    "LABEL=metad\n",
    "FILE=HILLS_LAMBDA  # read in the HILLS file\n",
    "RESTART=YES\n",
    "... METAD\n",
    "\n",
    "PRINT STRIDE=1 ARG=lambda,metad.bias FILE=COLVAR_REWEIGHT\n",
    "\n",
    "rw: REWEIGHT_BIAS TEMP=298\n",
    "PRINT ARG=lambda,rw FILE=weights.dat\n",
    "\n",
    "HISTOGRAM ...\n",
    "ARG=lambda\n",
    "LOGWEIGHTS=rw\n",
    "GRID_MIN=0\n",
    "GRID_MAX=5\n",
    "GRID_BIN=5\n",
    "CLEAR=1000\n",
    "NORMALIZATION=true\n",
    "KERNEL=DISCRETE\n",
    "LABEL=hhh\n",
    "... HISTOGRAM\n",
    "\n",
    "DUMPGRID GRID=hhh FILE=histograms/hist.dat STRIDE=1000\n",
    "\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocational-zoning",
   "metadata": {},
   "source": [
    "With `plumed_reweight.dat`, we run the plumed driver, after which the files `COLVAR_REWEIGHT`, `weights.dat` and all the files in the pre-existing folder `histograms` are generated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "freelance-funds",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLUMED: PLUMED is starting\n",
      "PLUMED: Version: 2.8.0-dev (git: 63008b018) compiled on Nov 21 2020 at 02:44:56\n",
      "PLUMED: Please cite these papers when using PLUMED [1][2]\n",
      "PLUMED: For further information see the PLUMED web page at http://www.plumed.org\n",
      "PLUMED: Root: /Users/Wei-TseHsu/Documents/Software/PLUMED/plumed2/\n",
      "PLUMED: For installed feature, see /Users/Wei-TseHsu/Documents/Software/PLUMED/plumed2//src/config/config.txt\n",
      "PLUMED: Molecular dynamics engine: driver\n",
      "PLUMED: Precision of reals: 8\n",
      "PLUMED: Running over 1 node\n",
      "PLUMED: Number of threads: 1\n",
      "PLUMED: Cache line size: 512\n",
      "PLUMED: Number of atoms: 0\n",
      "PLUMED: File suffix: \n",
      "PLUMED: FILE: plumed_reweight.dat\n",
      "PLUMED: Action READ\n",
      "PLUMED:   with label lambda\n",
      "PLUMED:   with stride 1\n",
      "PLUMED:   reading data from file COLVAR\n",
      "PLUMED:   reading value lambda and storing as lambda\n",
      "PLUMED: Action METAD\n",
      "PLUMED:   with label metad\n",
      "PLUMED:   with arguments lambda\n",
      "PLUMED:   added component to this action:  metad.bias \n",
      "PLUMED:   Gaussian width  0.010000  Gaussian height 0.000000\n",
      "PLUMED:   Gaussian deposition pace 50000000\n",
      "PLUMED:   Gaussian file HILLS_LAMBDA\n",
      "PLUMED:   Well-Tempered Bias Factor 50.000000\n",
      "PLUMED:   Hills relaxation time (tau) inf\n",
      "PLUMED:   KbT 2.477710\n",
      "PLUMED:   Grid min 0\n",
      "PLUMED:   Grid max 5\n",
      "PLUMED:   Grid bin 5\n",
      "PLUMED:   Grid uses spline interpolation\n",
      "PLUMED:   WARNING: Using a METAD with a Grid Spacing larger than half of the Gaussians width (SIGMA) can produce artifacts\n",
      "PLUMED:   Restarting from HILLS_LAMBDA:      250000 Gaussians read\n",
      "PLUMED:   Bibliography [3][4]\n",
      "PLUMED: Action PRINT\n",
      "PLUMED:   with label @2\n",
      "PLUMED:   with stride 1\n",
      "PLUMED:   with arguments lambda metad.bias\n",
      "PLUMED:   on file COLVAR_REWEIGHT\n",
      "PLUMED:   with format  %f\n",
      "PLUMED: Action REWEIGHT_BIAS\n",
      "PLUMED:   with label rw\n",
      "PLUMED:   with arguments metad.bias\n",
      "PLUMED: Action PRINT\n",
      "PLUMED:   with label @4\n",
      "PLUMED:   with stride 1\n",
      "PLUMED:   with arguments lambda rw\n",
      "PLUMED:   on file weights.dat\n",
      "PLUMED:   with format  %f\n",
      "PLUMED: Action HISTOGRAM\n",
      "PLUMED:   with label hhh\n",
      "PLUMED:   with stride 1\n",
      "PLUMED:   with arguments lambda\n",
      "PLUMED:   clearing grid every 1000 steps \n",
      "PLUMED:   reweighting using weights from rw \n",
      "PLUMED:   grid of 6 equally spaced points between (0) and (5)\n",
      "PLUMED: Action DUMPGRID\n",
      "PLUMED:   with label @6\n",
      "PLUMED:   with stride 1000\n",
      "PLUMED:   outputting grid calculated by action hhh to file named histograms/hist.dat with format %f \n",
      "PLUMED:   outputting data for replicas 0 END FILE: plumed_reweight.dat\n",
      "PLUMED: Timestep: 1.000000\n",
      "PLUMED: KbT has not been set by the MD engine\n",
      "PLUMED: It should be set by hand where needed\n",
      "PLUMED: Relevant bibliography:\n",
      "PLUMED:   [1] The PLUMED consortium, Nat. Methods 16, 670 (2019)\n",
      "PLUMED:   [2] Tribello, Bonomi, Branduardi, Camilloni, and Bussi, Comput. Phys. Commun. 185, 604 (2014)\n",
      "PLUMED:   [3] Laio and Parrinello, PNAS 99, 12562 (2002)\n",
      "PLUMED:   [4] Barducci, Bussi, and Parrinello, Phys. Rev. Lett. 100, 020603 (2008)\n",
      "PLUMED: Please read and cite where appropriate!\n",
      "PLUMED: Finished setup\n",
      "PLUMED:                                               Cycles        Total      Average      Minimum      Maximum\n",
      "PLUMED:                                                    1    12.075269    12.075269    12.075269    12.075269\n",
      "PLUMED: 1 Prepare dependencies                        250001     0.212924     0.000001     0.000001     0.000072\n",
      "PLUMED: 2 Sharing data                                250001     0.009964     0.000000     0.000000     0.000020\n",
      "PLUMED: 3 Waiting for data                            250001     0.009614     0.000000     0.000000     0.000021\n",
      "PLUMED: 4 Calculating (forward loop)                  250001     0.887879     0.000004     0.000003     0.000076\n",
      "PLUMED: 5 Applying (backward loop)                    250001     0.171909     0.000001     0.000001     0.000060\n",
      "PLUMED: 6 Update                                      250001     3.424490     0.000014     0.000010     0.003362\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source /Users/Wei-TseHsu/Documents/Software/PLUMED/plumed2/sourceme.sh\n",
    "export PLUMED_MAXBACKUP=10000\n",
    "\n",
    "plumed driver --plumed plumed_reweight.dat --noatoms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "played-telescope",
   "metadata": {},
   "source": [
    "## 3. Perform block averaging on the histograms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quiet-bahrain",
   "metadata": {},
   "source": [
    "At this point, there are 250 histogram files stored in the folder `histograms`. (In the 5 ns simulation, there are 2500000 steps (dt=0.002 ps). Since the `STRIDE` (the logging frequencey of `COLVAR`) used in metadynamics was 10 steps and the block size was 1000, the actual block size was 10000 steps, leading to 250 blocks in this case.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiac-length",
   "metadata": {},
   "source": [
    "As mentioned in the masterclass, with the weighted histogram of each block, the expectation and variance of the weighted average can be expressed as below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "balanced-calvin",
   "metadata": {},
   "source": [
    "$$\\mathbb{E}(\\overline{X}_w) = \\overline{X} \\qquad \\textrm{and} \\qquad \\textrm{var}(\\overline{X}_w) = \\frac{\\sum_{i=1}^N w_i^2 (X_i - \\overline{X}_w)^2 }{(\\sum_{i=1}^N w_i)^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "available-religion",
   "metadata": {},
   "source": [
    "To calculate the expectation and the variance (hence the uncertainty, which is the square root of the variance), we defined the following to functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "defensive-crawford",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_histogram(hist_file):\n",
    "    data = np.loadtxt(hist_file)\n",
    "    with open(hist_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            if line.startswith('#! SET normalisation'):\n",
    "                norm = float(line.split()[-1])\n",
    "    hist = data[:, -1]  # normalized probability: the last column\n",
    "    \n",
    "    return norm, hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "through-special",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_free_energy(hist_dir, hist_files):\n",
    "    # Step 1: Calculate the average of the weighted histogram for each gridded CV value\n",
    "    w_sum, avg = 0, 0\n",
    "    for f in hist_files:\n",
    "        norm, hist = read_histogram(f'{hist_dir}/{f}')\n",
    "        w_sum += norm\n",
    "        avg += norm * hist\n",
    "    avg = avg / w_sum\n",
    "    \n",
    "    # Step 2: Calculate the uncertainty of each gridded CV value\n",
    "    error = 0\n",
    "    for f in hist_files:\n",
    "        norm, hist = read = read_histogram(f'{hist_dir}/{f}')\n",
    "        error += norm * norm * (hist - avg) ** 2\n",
    "    error = np.sqrt(error / (w_sum **2))\n",
    "        \n",
    "    # Step 3: Conver to the uncertainty in free energy \n",
    "    fes = -np.log(avg)    # units: kT\n",
    "    f_err = error / avg   # units: kT\n",
    "    \n",
    "    return fes, f_err"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "level-active",
   "metadata": {},
   "source": [
    "Then, we calculate the free energy difference and the corresponding uncertainty as below. (Block size: 10000 simulation steps, or 20 ps.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "infinite-dance",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob('histograms/*hist.dat')\n",
    "fes, f_err = calculate_free_energy('.', files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "mineral-driving",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_CVs = 1\n",
    "CV_points = []\n",
    "for i in range(n_CVs):  \n",
    "    CV_points.append(np.transpose(np.loadtxt('histograms/hist.dat'))[i])\n",
    "\n",
    "output = open('fes_blocks.dat', 'w')\n",
    "for i in range(len(CV_points[0])):\n",
    "    CV_str = ''\n",
    "    for j in range(n_CVs):  \n",
    "        CV_str += f'{CV_points[j][i]: .3f}  '\n",
    "        output.write(f'{CV_str}   {fes[i]: .6f}   {f_err[i]: .6f}\\n')\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "specific-ordinance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.000      3.728798    0.057156\n",
      " 1.000      3.831584    0.053884\n",
      " 2.000      3.770377    0.048995\n",
      " 3.000      3.159967    0.029283\n",
      " 4.000      1.206905    0.004479\n",
      " 5.000      0.528123    0.007227\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cat fes_blocks.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "pleased-imagination",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The free energy difference is -3.200675 +/-  0.057611 kT.\n"
     ]
    }
   ],
   "source": [
    "print(f'The free energy difference is {fes[-1]-fes[0]: .6f} +/- {np.sqrt(f_err[-1] ** 2 + f_err[0] ** 2): .6f} kT.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developmental-lambda",
   "metadata": {},
   "source": [
    "## 4. Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "urban-money",
   "metadata": {},
   "source": [
    "We repeated the 5 ns 1D alchemical metadynamics of the argon atom disappearing from water 20 times to get 20 repetitions. Above is the data analysis of the first repetition. To assess the influence of the block size on the uncertainty, for each replicate, we varied the values of `STRIDE` and `CLEAR` and calcualted the uncertainty of the free energy difference. Below are the results for all the repetitions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "difficult-alert",
   "metadata": {},
   "source": [
    "<img src=https://i.imgur.com/X0Z3mqN.jpg width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unavailable-colorado",
   "metadata": {},
   "source": [
    "As shown above, 20 ps seems the be the most reasonable block size. Any blocks larger than this size are subject to larger noises. Therefore, for each repetition, we used 20 ps as the block size and calculated the free energy difference and its uncertainty. In the figure below, we use Gaussians to show the overlap between different repetitions, where the center and the width of the Gaussian are the free energy difference and its uncertainty, respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mighty-sense",
   "metadata": {},
   "source": [
    "<img src=https://i.imgur.com/56UI6s5.png width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corresponding-indicator",
   "metadata": {},
   "source": [
    "As a result, the mean and the standard deviation of the Gaussian centers shown above are -3.120 kT and 0.415 kT, respectively. Since the uncertainty estimated from each repetition was about 0.05 kT, the estimated uncertainty seems to be highly statistically inconsistent with each other. As a reference, the free energy estimated from a 5 ns expanded ensemble, which should be roughly the same performance as the 1D alchemical metadynamics, was -3.137 +/- 0.135 kT. According to our prior experiences, the truncation of the equilibration regime and the use of `UPDATE_UNTIL` in `METAD` to ensure rigorously static bias at the end of the simulaiton didn't seem to improve the situation. We therefore are wondering if you have suggestions on solving this statistical inconsistency. Thank you so much for your input!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "found-despite",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
