{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "demanding-issue",
   "metadata": {},
   "source": [
    "# Method 1 (PLUMED masterclass 21-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "through-twelve",
   "metadata": {},
   "source": [
    "## 1. Setting things up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satellite-gabriel",
   "metadata": {},
   "source": [
    "Below we import the required packages and set up the settings for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "detailed-agenda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floppy-correlation",
   "metadata": {},
   "source": [
    "Also, if you run this notebook for the second time, I suggest you clear the previous outputs to make the directory clean:\n",
    "```\n",
    "rm -r histograms/* COLVAR* HILLS* fes* bck* weights.dat *yaml\n",
    "cp ../input_files/* . \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specified-gravity",
   "metadata": {},
   "source": [
    "## 2. Reweight the data and generate histograms for block averaging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cosmetic-broadcasting",
   "metadata": {},
   "source": [
    "As a referene, the following is the content of the PLUMED input file used to run the alchemical metadynamics simulaiton.\n",
    "```\n",
    "lambda: EXTRACV NAME=lambda\n",
    "  \n",
    "METAD ...\n",
    "ARG=lambda\n",
    "SIGMA=0.01     # small SIGMA ensures that the Gaussian approaximate a delta function\n",
    "HEIGHT=1.2388545199729883   # 0.5 kT\n",
    "PACE=10        # should be nstexpanded\n",
    "GRID_MIN=0     # index of alchemical states starts from 0\n",
    "GRID_MAX=5     # we have 6 states in total\n",
    "GRID_BIN=5     # 5 bins between 6 states\n",
    "TEMP=298       # same as ref_t\n",
    "BIASFACTOR=50   \n",
    "LABEL=metad    \n",
    "FILE=HILLS_LAMBDA\n",
    "... METAD\n",
    "\n",
    "PRINT STRIDE=10 ARG=lambda,metad.bias FILE=COLVAR\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aerial-retirement",
   "metadata": {},
   "source": [
    "Below is the PLUMED input file for reweighting and generating histograms. As shown below, the block size was set by the `CLEAR` keyword in the `HISTOGRAM` action and the `STRIDE` keyword in the `DUMPGRID` action. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "requested-crazy",
   "metadata": {},
   "source": [
    "```\n",
    "lambda: READ FILE=COLVAR VALUES=lambda IGNORE_TIME IGNORE_FORCES\n",
    "  \n",
    "METAD ...\n",
    "\n",
    "ARG=lambda\n",
    "SIGMA=0.01\n",
    "HEIGHT=0     # kJ/mol\n",
    "PACE=50000000        # should be nstexpanded\n",
    "GRID_MIN=0\n",
    "GRID_MAX=5\n",
    "GRID_BIN=5\n",
    "TEMP=298\n",
    "BIASFACTOR=50\n",
    "LABEL=metad\n",
    "FILE=HILLS_LAMBDA  # read in the HILLS file\n",
    "RESTART=YES\n",
    "... METAD\n",
    "\n",
    "PRINT STRIDE=1 ARG=lambda,metad.bias FILE=COLVAR_REWEIGHT\n",
    "\n",
    "rw: REWEIGHT_BIAS TEMP=298\n",
    "PRINT ARG=lambda,rw FILE=weights.dat\n",
    "\n",
    "HISTOGRAM ...\n",
    "ARG=lambda\n",
    "LOGWEIGHTS=rw\n",
    "GRID_MIN=0\n",
    "GRID_MAX=5\n",
    "GRID_BIN=5\n",
    "CLEAR=1000\n",
    "NORMALIZATION=true\n",
    "KERNEL=DISCRETE\n",
    "LABEL=hhh\n",
    "... HISTOGRAM\n",
    "\n",
    "DUMPGRID GRID=hhh FILE=histograms/hist.dat STRIDE=1000\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocational-zoning",
   "metadata": {},
   "source": [
    "With `plumed_reweight.dat`, we run the plumed driver, after which the files `COLVAR_REWEIGHT`, `weights.dat` and all the files in the pre-existing folder `histograms` are generated. \n",
    "```\n",
    "export PLUMED_MAXBACKUP=10000\n",
    "plumed driver --plumed plumed_reweight.dat --noatoms\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "played-telescope",
   "metadata": {},
   "source": [
    "## 3. Perform block averaging on the histograms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quiet-bahrain",
   "metadata": {},
   "source": [
    "At this point, there are 250 histogram files stored in the folder `histograms`. (In the 5 ns simulation, there are 2500000 steps (dt=0.002 ps). Since the `STRIDE` (the logging frequencey of `COLVAR`) used in metadynamics was 10 steps and the block size was 1000, the actual block size was 10000 steps, leading to 250 blocks in this case.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiac-length",
   "metadata": {},
   "source": [
    "As mentioned in the masterclass, with the weighted histogram of each block, the expectation and variance of the weighted average can be expressed as below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "balanced-calvin",
   "metadata": {},
   "source": [
    "$$\\mathbb{E}(\\overline{X}_w) = \\overline{X} \\qquad \\textrm{and} \\qquad \\textrm{var}(\\overline{X}_w) = \\frac{\\sum_{i=1}^N w_i^2 (X_i - \\overline{X}_w)^2 }{(\\sum_{i=1}^N w_i)^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "available-religion",
   "metadata": {},
   "source": [
    "To calculate the expectation and the variance (hence the uncertainty, which is the square root of the variance), we defined the following to functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "defensive-crawford",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_histogram(hist_file):\n",
    "    data = np.loadtxt(hist_file)\n",
    "    with open(hist_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            if line.startswith('#! SET normalisation'):\n",
    "                norm = float(line.split()[-1])\n",
    "    hist = data[:, -1]  # normalized probability: the last column\n",
    "    \n",
    "    return norm, hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "through-special",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_free_energy(hist_dir, hist_files):\n",
    "    # Step 1: Calculate the average of the weighted histogram for each gridded CV value\n",
    "    w_sum, avg = 0, 0\n",
    "    for f in hist_files:\n",
    "        norm, hist = read_histogram(f'{hist_dir}/{f}')\n",
    "        w_sum += norm\n",
    "        avg += norm * hist\n",
    "    avg = avg / w_sum\n",
    "    \n",
    "    # Step 2: Calculate the uncertainty of each gridded CV value\n",
    "    error = 0\n",
    "    for f in hist_files:\n",
    "        norm, hist = read = read_histogram(f'{hist_dir}/{f}')\n",
    "        error += norm * norm * (hist - avg) ** 2\n",
    "    error = np.sqrt(error / (w_sum **2))\n",
    "        \n",
    "    # Step 3: Conver to the uncertainty in free energy \n",
    "    fes = -np.log(avg)    # units: kT\n",
    "    f_err = error / avg   # units: kT\n",
    "    \n",
    "    return fes, f_err"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "level-active",
   "metadata": {},
   "source": [
    "Then, we calculate the free energy difference and the corresponding uncertainty as below. (Block size: 10000 simulation steps, or 20 ps.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "infinite-dance",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob('histograms/*hist.dat')\n",
    "fes, f_err = calculate_free_energy('.', files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "mineral-driving",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_CVs = 1\n",
    "CV_points = []\n",
    "for i in range(n_CVs):  \n",
    "    CV_points.append(np.transpose(np.loadtxt('histograms/hist.dat'))[i])\n",
    "\n",
    "output = open('fes_blocks.dat', 'w')\n",
    "for i in range(len(CV_points[0])):\n",
    "    CV_str = ''\n",
    "    for j in range(n_CVs):  \n",
    "        CV_str += f'{CV_points[j][i]: .3f}  '\n",
    "        output.write(f'{CV_str}   {fes[i]: .6f}   {f_err[i]: .6f}\\n')\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "specific-ordinance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.000      3.728798    0.057156\n",
      " 1.000      3.831584    0.053884\n",
      " 2.000      3.770377    0.048995\n",
      " 3.000      3.159967    0.029283\n",
      " 4.000      1.206905    0.004479\n",
      " 5.000      0.528123    0.007227\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cat fes_blocks.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "pleased-imagination",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The free energy difference is -3.200675 +/-  0.057611 kT.\n"
     ]
    }
   ],
   "source": [
    "print(f'The free energy difference is {fes[-1]-fes[0]: .6f} +/- {np.sqrt(f_err[-1] ** 2 + f_err[0] ** 2): .6f} kT.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developmental-lambda",
   "metadata": {},
   "source": [
    "## 4. Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "urban-money",
   "metadata": {},
   "source": [
    "We repeated the 5 ns 1D alchemical metadynamics of the argon atom disappearing from water 20 times to get 20 repetitions. Above is the data analysis of the first repetition. To assess the influence of the block size on the uncertainty, for each replicate, we varied the values of `STRIDE` and `CLEAR` and calcualted the uncertainty of the free energy difference. Below are the results for all the repetitions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "difficult-alert",
   "metadata": {},
   "source": [
    "<img src=https://i.imgur.com/X0Z3mqN.jpg width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unavailable-colorado",
   "metadata": {},
   "source": [
    "As shown above, 20 ps seems the be the most reasonable block size. Any blocks larger than this size are subject to larger noises. Therefore, for each repetition, we used 20 ps as the block size and calculated the free energy difference and its uncertainty. In the figure below, we use Gaussians to show the overlap between different repetitions, where the center and the width of the Gaussian are the free energy difference and its uncertainty, respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mighty-sense",
   "metadata": {},
   "source": [
    "<img src=https://i.imgur.com/56UI6s5.png width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corresponding-indicator",
   "metadata": {},
   "source": [
    "As a result, the mean and the standard deviation of the Gaussian centers shown above are -3.120 kT and 0.415 kT, respectively. Since the uncertainty estimated from each repetition was about 0.05 kT, the estimated uncertainty seems to be highly statistically inconsistent with each other. As a reference, the free energy estimated from a 5 ns expanded ensemble, which should be roughly the same performance as the 1D alchemical metadynamics, was -3.137 +/- 0.135 kT. According to our prior experiences, the truncation of the equilibration regime and the use of `UPDATE_UNTIL` in `METAD` to ensure rigorously static bias at the end of the simulaiton didn't seem to improve the situation. We therefore are wondering if you have suggestions on solving this statistical inconsistency. Thank you so much for your input!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civic-frame",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
